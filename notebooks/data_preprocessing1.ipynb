{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing - fix dates\n",
    "\n",
    "df = pd.read_pickle('../data/motley-fool-data.pkl')\n",
    "df['date'].apply(type).value_counts()\n",
    "\n",
    "def normalize_date(val):\n",
    "    if isinstance(val, list):\n",
    "        return val[0] if len(val) > 0 else None\n",
    "    return val\n",
    "\n",
    "df[\"date\"] = df[\"date\"].apply(normalize_date)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "df = df.dropna(subset=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing - sentences\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from utils.preprocessing import process_row\n",
    "\n",
    "df = df[df[\"transcript\"].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "rows = df.to_dict(orient = \"records\")\n",
    "\n",
    "def run_parallel_processing():\n",
    "    with Pool(processes = cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(process_row, rows), total=len(rows), desc=\"Processing\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "results = run_parallel_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data in csv - Part 1\n",
    "\n",
    "sentence_data = [item for sublist in results for item in sublist]\n",
    "sentence_df = pd.DataFrame(sentence_data)\n",
    "sentence_df.to_csv(\"../data/parsed_sentences.csv\", index=False)\n",
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_news(ticker):\n",
    "    url = f\"https://finviz.com/quote.ashx?t={ticker}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print(\"Failed to fetch data\")\n",
    "        return []\n",
    "    \n",
    "    table = soup.find('table', class_ = 'fullview-news-outer')\n",
    "    \n",
    "    if table is None:\n",
    "        print(\"No news table found\")\n",
    "        return []\n",
    "    \n",
    "    rows = table.find_all('tr')\n",
    "    data, date = [], None\n",
    "    \n",
    "    for row in rows:\n",
    "        dt_raw = row.td.text.strip()\n",
    "        title = row.a.text.strip()\n",
    "        if len(dt_raw.split()) == 2:\n",
    "            date, time = dt_raw.split()\n",
    "            if date == \"Today\":\n",
    "                curr_date = datetime.now().date()\n",
    "            elif date == \"Yesterday\":\n",
    "                curr_date = (datetime.now() - timedelta(days=1)).date()\n",
    "            else:\n",
    "                curr_date = datetime.strptime(date, '%b-%d-%y').date()\n",
    "        else:\n",
    "            time = dt_raw\n",
    "        if curr_date:\n",
    "            data.append({'ticker': ticker, 'date': str(curr_date), 'time': time, 'text': title})\n",
    "    \n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "tables = pd.read_html(url)\n",
    "sp500_table = tables[0]\n",
    "sp500_tickers = sp500_table['Symbol'].tolist()\n",
    "sp500_tickers = [ticker.replace('.', '-') for ticker in sp500_tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "all_news = []\n",
    "\n",
    "for ticker in sp500_tickers:\n",
    "    time.sleep(random.uniform(1.0, 2.5))\n",
    "    all_news.extend(scrape_news(ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_news).to_csv(\"../data/finviz_headlines2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = pd.read_csv('../data/parsed_sentences.csv')\n",
    "headlines = pd.read_csv('../data/finviz_headlines.csv')\n",
    "\n",
    "transcripts = transcripts[[\"text\", \"date\", \"ticker\"]]\n",
    "headlines = headlines[[\"text\", \"date\", \"ticker\"]]\n",
    "\n",
    "transcripts = transcripts.sample(n = 25000, random_state=42)\n",
    "\n",
    "combine = pd.concat([transcripts, headlines], ignore_index=True)\n",
    "combine.dropna(subset=['text', 'date', 'ticker'], inplace=True)\n",
    "combine.to_csv('../data/combined.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../models/finbert-finetuned1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/finbert-finetuned1\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/combined.csv')\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "batch_size = 64\n",
    "preds, probs = [], []\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Classifying\"):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "    enc = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        batch_probs = softmax(logits, dim=1)\n",
    "        preds.extend(batch_probs.argmax(dim=1).cpu().tolist())\n",
    "        probs.extend(batch_probs.cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_label'] = preds\n",
    "df[['positive', 'neutral', 'negative']] = pd.DataFrame(probs)\n",
    "\n",
    "grouped = df.groupby(['ticker', 'date']).agg({\n",
    "    'text': lambda x: list(x),\n",
    "    'ticker': lambda x: list(x),\n",
    "    'date': lambda x: list(x),\n",
    "    'sentiment_label': lambda x: list(x),\n",
    "    'positive': lambda x: list(x),\n",
    "    'neutral': lambda x: list(x),\n",
    "    'negative': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "grouped.to_csv(\"../data/grouped_by_date.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/grouped_by_date.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (FinBERT Global)",
   "language": "python",
   "name": "finbert-global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
