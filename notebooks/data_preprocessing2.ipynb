{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_lines = []\n",
    "\n",
    "with open(\"../data/full_dataset-release.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "i = 0\n",
    "while i < len(lines):\n",
    "    line = lines[i].strip()\n",
    "\n",
    "    if line and line[0].isdigit() and i + 1 < len(lines):\n",
    "        next_line = lines[i + 1].strip()\n",
    "\n",
    "        if next_line.startswith(\",\"):\n",
    "            combined = line + next_line\n",
    "            fixed_lines.append(combined)\n",
    "            i += 2\n",
    "            continue\n",
    "\n",
    "    fixed_lines.append(line)\n",
    "    i += 1\n",
    "\n",
    "with open(\"../data/fixed_dataset.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in fixed_lines:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n0/gj667hr12xq0lkb3bn4rjxkc0000gp/T/ipykernel_9166/2142268492.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/fixed_dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/fixed_dataset.csv')\n",
    "\n",
    "df.columns = [\n",
    "    'id', 'text', 'ticker', 'date', 'price',\n",
    "    'return_1d', 'return_2d', 'return_3d', 'return_7d',\n",
    "    'volume', 'volatility_10d', 'volatility_30d',\n",
    "    'lstm_sentiment', 'textblob_sentiment'\n",
    "]\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce', dayfirst=True)\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "df = df.drop(columns=['id', 'lstm_sentiment', 'textblob_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>return_1d</th>\n",
       "      <th>return_2d</th>\n",
       "      <th>return_3d</th>\n",
       "      <th>return_7d</th>\n",
       "      <th>volume</th>\n",
       "      <th>volatility_10d</th>\n",
       "      <th>volatility_30d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @robertoglezcano: @amazon  #Patents Show Fl...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>823.48</td>\n",
       "      <td>0.008379</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>-0.001263</td>\n",
       "      <td>3137196.0</td>\n",
       "      <td>13.447</td>\n",
       "      <td>16.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@FAME95FM1 Jamaicans make money with @Payoneer...</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>39.78</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>9100057.0</td>\n",
       "      <td>18.769</td>\n",
       "      <td>16.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@CBSi Jamaicans make money with @Payoneer @Pay...</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>39.78</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>9100057.0</td>\n",
       "      <td>18.769</td>\n",
       "      <td>16.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Hitz92fm Jamaicans make money with @Payoneer ...</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>39.78</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.012318</td>\n",
       "      <td>0.054801</td>\n",
       "      <td>9100057.0</td>\n",
       "      <td>18.769</td>\n",
       "      <td>16.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @loadsofvans: Retweet this post &amp;amp; follo...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>823.48</td>\n",
       "      <td>0.008379</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>-0.001263</td>\n",
       "      <td>3137196.0</td>\n",
       "      <td>13.447</td>\n",
       "      <td>16.992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  ticker       date  \\\n",
       "0  RT @robertoglezcano: @amazon  #Patents Show Fl...  Amazon 2017-01-31   \n",
       "1  @FAME95FM1 Jamaicans make money with @Payoneer...  PayPal 2017-01-31   \n",
       "2  @CBSi Jamaicans make money with @Payoneer @Pay...  PayPal 2017-01-31   \n",
       "3  @Hitz92fm Jamaicans make money with @Payoneer ...  PayPal 2017-01-31   \n",
       "4  RT @loadsofvans: Retweet this post &amp; follo...  Amazon 2017-01-31   \n",
       "\n",
       "    price  return_1d  return_2d  return_3d  return_7d     volume  \\\n",
       "0  823.48   0.008379   0.014924   0.014924  -0.001263  3137196.0   \n",
       "1   39.78   0.002011   0.012318   0.012318   0.054801  9100057.0   \n",
       "2   39.78   0.002011   0.012318   0.012318   0.054801  9100057.0   \n",
       "3   39.78   0.002011   0.012318   0.012318   0.054801  9100057.0   \n",
       "4  823.48   0.008379   0.014924   0.014924  -0.001263  3137196.0   \n",
       "\n",
       "   volatility_10d  volatility_30d  \n",
       "0          13.447          16.992  \n",
       "1          18.769          16.099  \n",
       "2          18.769          16.099  \n",
       "3          18.769          16.099  \n",
       "4          13.447          16.992  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "chunk_size = 50000\n",
    "num_chunks = 7\n",
    "\n",
    "chunks = [df[i * chunk_size : (i + 1) * chunk_size] for i in range(num_chunks)]\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv(f\"./data/df_chunk_{i+1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanvu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"../models/finbert-finetuned1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/finbert-finetuned1\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Chunk 4: 100%|██████████| 782/782 [49:32<00:00,  3.80s/it] \n",
      "Classifying Chunk 5: 100%|██████████| 782/782 [50:52<00:00,  3.90s/it]\n",
      "Classifying Chunk 6: 100%|██████████| 782/782 [45:24<00:00,  3.48s/it]\n",
      "Classifying Chunk 7: 100%|██████████| 782/782 [44:24<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_paths = [f\"./data/df_chunk_{i}.csv\" for i in range(4, 8)]\n",
    "\n",
    "for i, path in enumerate(chunk_paths):\n",
    "    i = i + 3\n",
    "    df = pd.read_csv(path)\n",
    "    texts = df['text'].tolist()\n",
    "\n",
    "    batch_size = 64\n",
    "    preds, probs = [], []\n",
    "\n",
    "    for j in tqdm(range(0, len(texts), batch_size), desc=f\"Classifying Chunk {i+1}\"):\n",
    "        batch_texts = texts[j:j + batch_size]\n",
    "        enc = tokenizer(batch_texts, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            batch_probs = softmax(logits, dim=1)\n",
    "            preds.extend(batch_probs.argmax(dim=1).cpu().tolist())\n",
    "            probs.extend(batch_probs.cpu().tolist())\n",
    "            \n",
    "    df = df.iloc[:len(preds)].copy()\n",
    "            \n",
    "    df['sentiment_label'] = preds\n",
    "    df['positive'] = [p[0] for p in probs]\n",
    "    df['neutral'] = [p[1] for p in probs]\n",
    "    df['negative'] = [p[2] for p in probs]\n",
    "\n",
    "    df.to_csv(f\"./data/df_chunk_{i+1}_with_sentiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "all_chunks = pd.concat(\n",
    "    [pd.read_csv(f) for f in sorted(glob.glob(\"./data/df_chunk_*_with_sentiment.csv\"))],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = all_chunks.groupby(['ticker', 'date']).agg({\n",
    "    'text': lambda x: list(x),\n",
    "    'sentiment_label': lambda x: list(x),\n",
    "    'positive': lambda x: list(x),\n",
    "    'neutral': lambda x: list(x),\n",
    "    'negative': lambda x: list(x),\n",
    "    'price': lambda x: list(x),\n",
    "    'volume': lambda x: list(x), \n",
    "    'volatility_10d': lambda x: list(x), \n",
    "    'volatility_30d': lambda x: list(x),\n",
    "    'return_1d': lambda x: list(x),\n",
    "    'return_2d': lambda x: list(x),\n",
    "    'return_3d': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "daily_sentiment = all_chunks.groupby([\"ticker\", \"date\"]).agg({\n",
    "    'text': lambda x: list(x),\n",
    "    \"positive\": \"mean\",\n",
    "    \"neutral\": \"mean\",\n",
    "    \"negative\": \"mean\",\n",
    "    'price': \"first\",\n",
    "    'volume': \"first\", \n",
    "    'volatility_10d': \"first\", \n",
    "    'volatility_30d': \"first\",\n",
    "    \"return_1d\": \"first\",\n",
    "    \"return_2d\": \"first\",\n",
    "    \"return_3d\": \"first\"\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment = daily_sentiment.dropna(subset=['positive', 'neutral', 'negative', 'return_1d'])\n",
    "daily_sentiment = daily_sentiment.sort_values(by=['ticker', 'date'])\n",
    "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>price</th>\n",
       "      <th>volume</th>\n",
       "      <th>volatility_10d</th>\n",
       "      <th>volatility_30d</th>\n",
       "      <th>return_1d</th>\n",
       "      <th>return_2d</th>\n",
       "      <th>return_3d</th>\n",
       "      <th>ticker_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21CF</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>[RT @21CF: 21CF internal memo from Executive C...</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>31.38</td>\n",
       "      <td>5170587.0</td>\n",
       "      <td>16.864</td>\n",
       "      <td>14.768</td>\n",
       "      <td>-0.000319</td>\n",
       "      <td>-0.002868</td>\n",
       "      <td>-0.002868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21CF</td>\n",
       "      <td>2017-04-29</td>\n",
       "      <td>[RT @21CF: Read what @Gotham star @ben_mckenzi...</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.999737</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>30.54</td>\n",
       "      <td>6681951.0</td>\n",
       "      <td>17.751</td>\n",
       "      <td>16.189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.006549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASOS</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>[RT @n76seary: RT @StudentBunker: #FreebieFrid...</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999649</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>5266.00</td>\n",
       "      <td>342823.0</td>\n",
       "      <td>32.807</td>\n",
       "      <td>28.367</td>\n",
       "      <td>-0.012533</td>\n",
       "      <td>0.008355</td>\n",
       "      <td>0.008355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASOS</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>[ASOS SALON Pretty Floral Soft Midi with Embel...</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.999595</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>5267.00</td>\n",
       "      <td>301346.0</td>\n",
       "      <td>26.819</td>\n",
       "      <td>28.350</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.012721</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASOS</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>[GUADALUPE PASS AMOS,TX (GDP) ASOS reports gus...</td>\n",
       "      <td>0.222588</td>\n",
       "      <td>0.626411</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>5432.00</td>\n",
       "      <td>608408.0</td>\n",
       "      <td>10.720</td>\n",
       "      <td>21.310</td>\n",
       "      <td>-0.002577</td>\n",
       "      <td>-0.018962</td>\n",
       "      <td>-0.018962</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date                                               text  \\\n",
       "0   21CF 2017-01-31  [RT @21CF: 21CF internal memo from Executive C...   \n",
       "1   21CF 2017-04-29  [RT @21CF: Read what @Gotham star @ben_mckenzi...   \n",
       "2   ASOS 2017-01-31  [RT @n76seary: RT @StudentBunker: #FreebieFrid...   \n",
       "3   ASOS 2017-02-01  [ASOS SALON Pretty Floral Soft Midi with Embel...   \n",
       "4   ASOS 2017-02-28  [GUADALUPE PASS AMOS,TX (GDP) ASOS reports gus...   \n",
       "\n",
       "   positive   neutral  negative    price     volume  volatility_10d  \\\n",
       "0  0.000167  0.999619  0.000214    31.38  5170587.0          16.864   \n",
       "1  0.000117  0.999737  0.000147    30.54  6681951.0          17.751   \n",
       "2  0.000176  0.999649  0.000175  5266.00   342823.0          32.807   \n",
       "3  0.000231  0.999595  0.000175  5267.00   301346.0          26.819   \n",
       "4  0.222588  0.626411  0.151001  5432.00   608408.0          10.720   \n",
       "\n",
       "   volatility_30d  return_1d  return_2d  return_3d  ticker_encoded  \n",
       "0          14.768  -0.000319  -0.002868  -0.002868               0  \n",
       "1          16.189   0.000000   0.000655   0.006549               0  \n",
       "2          28.367  -0.012533   0.008355   0.008355               1  \n",
       "3          28.350  -0.000190  -0.012721   0.008164               1  \n",
       "4          21.310  -0.002577  -0.018962  -0.018962               1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "daily_sentiment['ticker_encoded'] = le.fit_transform(daily_sentiment['ticker'])\n",
    "    \n",
    "daily_sentiment = daily_sentiment.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "daily_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment['return_1d'] *= 100\n",
    "daily_sentiment['return_2d'] *= 100\n",
    "\n",
    "daily_sentiment[\"price_volatility_ratio\"] = daily_sentiment['price'] / daily_sentiment['volatility_30d']\n",
    "daily_sentiment[\"volume_volatility_ratio\"] = daily_sentiment['volume'] / daily_sentiment['volatility_30d']\n",
    "daily_sentiment[\"volatility_diff\"] = daily_sentiment['volatility_10d'] - daily_sentiment['volatility_30d']\n",
    "daily_sentiment['day_of_week'] = pd.to_datetime(daily_sentiment['date']).dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.to_csv(\"../data/full_sentiment_dataset.csv\", index=False)\n",
    "daily_sentiment.to_csv(\"../data/full_sentiment_dataset_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (FinBERT Global)",
   "language": "python",
   "name": "finbert-global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
